{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced by John Bickers; and Dagny\n",
      "\n",
      "CRIME AND PUNISHMENT\n",
      "\n",
      "By Fyodor Dostoevsky\n",
      "\n",
      "\n",
      "Translated By Constance Garnett\n",
      "\n",
      "\n",
      "TRANSLATOR'S PREFACE\n",
      "\n",
      "A few words \n"
     ]
    }
   ],
   "source": [
    "## CODE IDEAS FOR HMW 2, Exploratory exercise for sentiment analysis\n",
    "# finding adverb and adjective phrases, and computing basic statistics\n",
    "\n",
    "# importing required nltk libraries\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# loading our corpus (for this example: \"Crime and Punishment,\" by F. Dostoevsky)\n",
    "f = open('CrimeAndPunishment.txt')\n",
    "text = f.read()\n",
    "print(text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Produced by John Bickers; and Dagny\\n\\nCRIME AND PUNISHMENT\\n\\nBy Fyodor Dostoevsky\\n\\n\\nTranslated By Constance Garnett\\n\\n\\nTRANSLATOR'S PREFACE\\n\\nA few words about Dostoevsky himself may help the English reader to\\nunderstand his work.\", 'Dostoevsky was the son of a doctor.', 'His parents were very hard-working\\nand deeply religious people, but so poor that they lived with their five\\nchildren in only two rooms.', 'The father and mother spent their evenings\\nin reading aloud to their children, generally from books of a serious\\ncharacter.', 'Though always sickly and delicate Dostoevsky came out third in the\\nfinal examination of the Petersburg school of Engineering.', 'There he had\\nalready begun his first work, \"Poor Folk.\"', 'This story was published by the poet Nekrassov in his review and\\nwas received with acclamations.', 'The shy, unknown youth found himself\\ninstantly something of a celebrity.', 'A brilliant and successful career\\nseemed to open before him, but those hopes were soon dashed.', 'In 1849 he\\nwas arrested.']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, as explained in the Labs\n",
    "# Separate the text into sentences first\n",
    "textsplit = nltk.sent_tokenize(text)\n",
    "print(textsplit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'words', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.'], ['Dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doctor', '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14723"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the word tokenizer to each sentence\n",
    "tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "print(tokentext[:2])\n",
    "#the output is a list of strings that contains the sentences\n",
    "type(tokentext)\n",
    "len(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Produced', 'VBN'), ('by', 'IN'), ('John', 'NNP'), ('Bickers', 'NNP'), (';', ':'), ('and', 'CC'), ('Dagny', 'NNP'), ('CRIME', 'NNP'), ('AND', 'NNP'), ('PUNISHMENT', 'NNP'), ('By', 'IN'), ('Fyodor', 'NNP'), ('Dostoevsky', 'NNP'), ('Translated', 'NNP'), ('By', 'IN'), ('Constance', 'NNP'), ('Garnett', 'NNP'), ('TRANSLATOR', 'NNP'), (\"'S\", 'POS'), ('PREFACE', 'NNP'), ('A', 'NNP'), ('few', 'JJ'), ('words', 'NNS'), ('about', 'IN'), ('Dostoevsky', 'NNP'), ('himself', 'PRP'), ('may', 'MD'), ('help', 'VB'), ('the', 'DT'), ('English', 'JJ'), ('reader', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('his', 'PRP$'), ('work', 'NN'), ('.', '.')], [('Dostoevsky', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('son', 'NN'), ('of', 'IN'), ('a', 'DT'), ('doctor', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "## POS Tagging, to retrieve adjective (JJs) and adverb (RBs) tags\n",
    "\n",
    "# use the Stanford POS tagger to POS tag tokens of each sentence\n",
    "# this is the default tagger in nltk\n",
    "taggedtext = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adjective phrases:  ['very hard-working ', 'deeply religious ', 'so poor ', 'most widely read ', 'so much more ', 'exceptionally hot ', 'acutely aware ', 'most .... ', 'too much ', '_that_ serious ']\n",
      "Top adjective phrases with frequency: \n",
      "too much  23\n",
      "so much  18\n",
      "very much  15\n",
      "too ....  13\n",
      "very good  11\n",
      "once more  10\n",
      "very glad  9\n",
      "quite different  9\n",
      "most likely  8\n",
      "very likely  8\n",
      "not worth  8\n",
      "very little  7\n",
      "very important  7\n",
      "very minute  6\n",
      "very pale  6\n",
      "too late  6\n",
      "as much  6\n",
      "too great  6\n",
      "very different  5\n",
      "not afraid  5\n",
      "so many  5\n",
      "most important  5\n",
      "not so much  5\n",
      "very anxious  5\n",
      "very strange  4\n",
      "very next  4\n",
      "so little  4\n",
      "very difficult  4\n",
      "as clear  4\n",
      "so strange  4\n",
      "very young  4\n",
      "not asleep  4\n",
      "hardly able  4\n",
      "Very good  4\n",
      "not right  4\n",
      "so stupid  4\n",
      "not drunk  4\n",
      "not mad  4\n",
      "not delirious  4\n",
      "most interesting  4\n",
      "very weak  3\n",
      "just such  3\n",
      "very clean  3\n",
      "very bad  3\n",
      "quite right  3\n",
      "once ....  3\n",
      "very poor  3\n",
      "very great  3\n",
      "very busy  3\n",
      "very short  3\n",
      "Length of adjective phrase sentences:  1636\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, chapter on Information Extraction--Chunking (https://www.nltk.org/book/ch07.html)\n",
    "\n",
    "# Using CHUNKING to parse sentences \n",
    "# to look for \"adjective phrases\", i.e. phrases (or chunks) that have adverbs and adjectives ('RB'+'JJ')\n",
    "# First step: writing a grammar that defines the POS in the chunk\n",
    "# we name this grammar \"ADJPH\" (\"ADJective PHrase\") using regexes \n",
    "\n",
    "import re\n",
    "grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\"\n",
    "# This regex reads as: \"find groups (\"< >\") of RBs (adverbs) together with groups of JJs (adjectives), with groups defineds as\n",
    "# RBs with any ending (the \".\" is a placeholder or wildcard for the \"R\" and the \"S\" at the end of RBR and RBS, \n",
    "# while \"?\" indicates \"optional character\" so RB can be found alone as well). Same regex operators apply to JJs.\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\n",
    "\n",
    "adjph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adj.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADJPH':\n",
    "                adjph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adjective_phrases = []\n",
    "for sent in adjph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adjective_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adjective phrases: ', adjective_phrases[:10])\n",
    "\n",
    "\n",
    "# Following our NLTK textbook, chapter 1 on Language Processing (https://www.nltk.org/book/ch01.html)\n",
    "\n",
    "## FREQUENCY DISTRIBUTIONS\n",
    "# Top 50 adjective phrases\n",
    "freq_adjph = nltk.FreqDist(adjective_phrases)\n",
    "\n",
    "print('Top adjective phrases by frequency: ')\n",
    "for word, freq in freq_adjph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adjective phrase sentences: ', len(adjph_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adverb phrases:  ['always sickly ', 'as soon ', 'still probably ', 'so much ', 'so completely ', 'not only ', 'there now ', 'along not ', 'so badly ', 'not far ']\n",
      "Top adverb phrases with frequency: \n",
      "just now  47\n",
      "as soon  33\n",
      "not even  30\n",
      "not so  19\n",
      "only just  17\n",
      "as well  16\n",
      "very well  14\n",
      "so long  13\n",
      "not quite  13\n",
      "so much  12\n",
      "not only  12\n",
      "not yet  12\n",
      "very much  12\n",
      "As soon  10\n",
      "long ago  10\n",
      "down again  9\n",
      "even now  9\n",
      "so far  8\n",
      "not very  8\n",
      "as far  7\n",
      "too soon  7\n",
      "n't even  7\n",
      "quite well  6\n",
      "n't quite  6\n",
      "very soon  6\n",
      "Very well  6\n",
      "not exactly  6\n",
      "not far  5\n",
      "so soon  5\n",
      "not now  5\n",
      "away somewhere  5\n",
      "Quite so  5\n",
      "not merely  5\n",
      "so completely  4\n",
      "there now  4\n",
      "so directly  4\n",
      "not simply  4\n",
      "perhaps not  4\n",
      "too far  4\n",
      "long before  4\n",
      "just before  4\n",
      "perfectly well  4\n",
      "almost always  4\n",
      "far away  4\n",
      "only now  4\n",
      "far too  4\n",
      "As far  4\n",
      "not here  3\n",
      "down beside  3\n",
      "sometimes even  3\n",
      "Length of adverb phrase sentences:  1189\n"
     ]
    }
   ],
   "source": [
    "# Now we look for \"adverb phrases\" or chunks that have 2 consecutive adverbs ('RB')\n",
    "# First step: writing a grammar that defines POS rules of the adverb phrase the chunk\n",
    "# we name this grammar \"ADVPH\" (\"ADVerb PHrase\")\n",
    "grammar_advph = \"ADVPH: {<RB>+<RB>}\"\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adv = nltk.RegexpParser(grammar_advph)\n",
    "\n",
    "advph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adv.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADVPH':\n",
    "                advph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adverb_phrases = []\n",
    "for sent in advph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adverb_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adverb phrases: ', adverb_phrases[:10])\n",
    "\n",
    "# top 50 adjective phrases\n",
    "freq_advph = nltk.FreqDist(adverb_phrases)\n",
    "\n",
    "print('Top adverb phrases by frequency: ')\n",
    "for word, freq in freq_advph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adverb phrase sentences: ', len(advph_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little 275\n",
      "last 274\n",
      "such 206\n",
      "old 198\n",
      "first 196\n",
      "other 188\n",
      "same 178\n",
      "more 172\n",
      "own 168\n",
      "great 162\n",
      "good 141\n",
      "strange 113\n",
      "young 112\n",
      "whole 109\n",
      "much 105\n",
      "right 97\n",
      "new 94\n",
      "long 91\n",
      "better 88\n",
      "least 85\n",
      ".... 75\n",
      "many 72\n",
      "certain 72\n",
      "open 65\n",
      "flat 64\n",
      "sure 62\n",
      "to-day 62\n",
      "dear 60\n",
      "next 60\n",
      "poor 56\n",
      "possible 56\n",
      "few 55\n",
      "sudden 54\n",
      "clear 54\n",
      "second 53\n",
      "full 53\n",
      "true 52\n",
      "afraid 50\n",
      "angry 49\n",
      "present 49\n",
      "ready 48\n",
      "Good 48\n",
      "terrible 47\n",
      "late 47\n",
      "pale 47\n",
      "different 46\n",
      "able 45\n",
      "dead 44\n",
      "stupid 41\n",
      "special 40\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adjective tokens\n",
    "\n",
    "adjective_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['JJ', 'JJR', 'JJS']: # adjective, comparative, superlative\n",
    "            if len(word)>1:\n",
    "                adjective_tokens.append(word)\n",
    "freq_adjective = nltk.FreqDist(adjective_tokens)\n",
    "\n",
    "for word, freq in freq_adjective.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not 1737\n",
      "n't 1065\n",
      "so 666\n",
      "too 500\n",
      "now 493\n",
      "very 436\n",
      "only 390\n",
      "again 390\n",
      "then 348\n",
      "once 340\n",
      "even 310\n",
      "here 289\n",
      "still 269\n",
      "just 268\n",
      "suddenly 262\n",
      "more 240\n",
      "away 222\n",
      "almost 208\n",
      "there 194\n",
      "perhaps 162\n",
      "Well 162\n",
      "quite 157\n",
      "never 148\n",
      "back 141\n",
      "always 136\n",
      "well 135\n",
      "as 134\n",
      "simply 131\n",
      "indeed 131\n",
      "down 131\n",
      "really 128\n",
      "up 125\n",
      "Then 123\n",
      "soon 102\n",
      "most 99\n",
      "already 94\n",
      "yet 91\n",
      "long 86\n",
      "alone 85\n",
      "rather 82\n",
      "much 77\n",
      "far 76\n",
      "So 76\n",
      "Here 73\n",
      "Now 71\n",
      "certainly 68\n",
      "together 65\n",
      "ever 65\n",
      "sometimes 63\n",
      "ago 60\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adverb tokens\n",
    "\n",
    "adverb_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['RB', 'RBR', 'RBS']: # adverb, comparative, superlative\n",
    "            if len(word)>1:\n",
    "                adverb_tokens.append(word)\n",
    "freq_adverb = nltk.FreqDist(adverb_tokens)\n",
    "\n",
    "for word, freq in freq_adverb.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO / YOUR TURN NOW!\n",
    "## NOUN EXTRACTION\n",
    "## VERB EXTRACTION\n",
    "## REMEMBER TO CHECK THE PENN POS TAGS LIST: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "## TO FIND ALL TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3416\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of POS tags combinations we can compare\n",
    "# We need to get the sentences back from the tagging exercise and run some stats\n",
    "\n",
    "# Create a list of original sentences from the ADJECTIVE phrase subset:\n",
    "adjph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in adjph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        adjph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2471\n"
     ]
    }
   ],
   "source": [
    "# Create a list of original sentences from the ADVERB phrase subset:\n",
    "advph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in advph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        advph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(advph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4102\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL STEP: Combine lists together to have a single list of adjective/adverb phrases:\n",
    "# Useful to know which sentences are heavy in qualifiers\n",
    "\n",
    "# create a new variable to store all adjective phrase sentences\n",
    "adv_adj_phrase_sentences = adjph_whole_sentences\n",
    "\n",
    "# iterate over adverb phrase sentences\n",
    "for sent in advph_whole_sentences:\n",
    "    # if a sentence is not in the adjective phrases list imported\n",
    "    if sent not in adv_adj_phrase_sentences:\n",
    "        # attach that sentence\n",
    "        adv_adj_phrase_sentences.append(sent)\n",
    "\n",
    "# print the lenght of the list (i.e. number of sentences with both adjective and adverb phrases)\n",
    "print(len(adv_adj_phrase_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.85607552808531\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, Writing Structural Programs chapter\n",
    "# section on Procedural vs Declarative style (http://www.nltk.org/book_1ed/ch04.html) \n",
    "\n",
    "## CORPUS STATISTICS--SENTENCES LENGTH\n",
    "\n",
    "# Calculating the average length of sentences in the entire corpus\n",
    "# from http://www.nltk.org/book_1ed/ch04.html\n",
    "total_corpus = sum(len(sent) for sent in textsplit) # remember: 'textsplit' is our text split into sentences\n",
    "print(total_corpus / len(textsplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.254266211604095\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average length of an adjective phrase sentence\n",
    "# We can then compare the average length of the adjective phrases to \n",
    "# the average sentences we calculated for all sentences in the corpus\n",
    "total_adjph_sentences = sum(len(sent) for sent in adjph_whole_sentences) # adjph_whole_sentences stores our adjective phrases\n",
    "print(total_adjph_sentences / len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
